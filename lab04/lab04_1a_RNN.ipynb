{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "compliant-welsh",
      "metadata": {
        "id": "compliant-welsh"
      },
      "source": [
        "# Lab 04 - Simple sentiment analysis with an RNN\n",
        "In this lab we will experiment with different architectures of Recurrent Neural Nets (RNN), but will also use pre-trained word embeddings and several experimental setups. The Python framework for this lab relies on PyTorch.\n",
        "\n",
        "This lab is based on the [popular PyTorch sentiment analysis tutorial by bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "kdqaAN9HARHd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 42
        },
        "id": "kdqaAN9HARHd",
        "outputId": "0dae8411-c208-4cba-fc57-2af40efd5ffc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a target=\"_blank\" href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2026/blob/main/lab04/Lab04_1a_RNN.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "colab_button = HTML(\n",
        "    '<a target=\"_blank\" href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2026/blob/main/lab04/Lab04_1a_RNN.ipynb\">'\n",
        "    '<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'\n",
        ")\n",
        "display(colab_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flexible-perspective",
      "metadata": {
        "id": "flexible-perspective"
      },
      "source": [
        "We'll be building a machine learning model to detect sentiment (i.e. detect if a sentence is positive or negative) using PyTorch. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$.\n",
        "\n",
        "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
        "\n",
        "Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$, (also known as a fully connected layer), to receive our predicted sentiment, $\\hat{y} = f(h_T)$.\n",
        "\n",
        "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros.\n",
        "\n",
        "![](https://github.com/surrey-nlp/NLP-2025/blob/main/lab04/assets/sentiment1.png?raw=1)\n",
        "\n",
        "**Note:** some layers and steps have been omitted from the diagram, but these will be explained later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "talented-adams",
      "metadata": {
        "id": "talented-adams"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip -q install torch datasets spacy tqdm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea336d7f-04a0-49b6-83e6-53bd3fe0392c",
      "metadata": {
        "id": "ea336d7f-04a0-49b6-83e6-53bd3fe0392c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "SEED = 1234\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "disturbed-developer",
      "metadata": {
        "id": "disturbed-developer"
      },
      "source": [
        "## Initialising the dataset\n",
        "\n",
        "In this lab, we use the IMDb dataset via the ðŸ¤— Hugging Face Datasets library.\n",
        "This library provides convenient access to many widely used NLP datasets.\n",
        "\n",
        "The IMDb dataset consists of 50,000 movie reviews, each labelled as either positive or negative.\n",
        "Each example contains:\n",
        "- a `text` field (the review)\n",
        "- a `label` field (0 for negative, 1 for positive)\n",
        "\n",
        "The dataset is automatically downloaded (if not already cached locally) and loaded\n",
        "into its canonical `train` and `test` splits.\n",
        "\n",
        "At this stage, the data is still raw text. Tokenisation, vocabulary construction,\n",
        "numericalisation, and padding are handled later in the preprocessing pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reflected-shelter",
      "metadata": {
        "id": "reflected-shelter"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# IMDB movie review dataset (binary sentiment)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_data = [(ex[\"label\"], ex[\"text\"]) for ex in dataset[\"train\"]]\n",
        "test_data  = [(ex[\"label\"], ex[\"text\"]) for ex in dataset[\"test\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_rWjY8gp6Tz8",
      "metadata": {
        "id": "_rWjY8gp6Tz8"
      },
      "source": [
        "We can see how many examples there are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437867eb-0f87-4a0f-95c9-aaf329579bed",
      "metadata": {
        "id": "437867eb-0f87-4a0f-95c9-aaf329579bed"
      },
      "outputs": [],
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b8b1cb-d717-434f-bfe8-95afab300de9",
      "metadata": {
        "id": "11b8b1cb-d717-434f-bfe8-95afab300de9"
      },
      "source": [
        "We can also check an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4da5b9c-85b4-42d3-a568-dde6a7e55c94",
      "metadata": {
        "id": "d4da5b9c-85b4-42d3-a568-dde6a7e55c94"
      },
      "outputs": [],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d499b8d1-e937-4613-9c0d-c36a11525af7",
      "metadata": {
        "id": "d499b8d1-e937-4613-9c0d-c36a11525af7"
      },
      "source": [
        "Note how samples in this data set have the label first and the data point second.\n",
        "\n",
        "To split the train set into a train and validation set, we'll use PyTorch's `random_split` utility.\n",
        "\n",
        "Note that when we initially imported PyTorch at the top of this notebook, we also set the seed using `manual_seed` to a constant, so the result of `random_split` will be reproducible in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61fc8ae6-1b2a-4864-ab55-e942ddb9bfb8",
      "metadata": {
        "id": "61fc8ae6-1b2a-4864-ab55-e942ddb9bfb8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "split_ratio = 0.7  # 70/30 split\n",
        "full_train = ListDataset(train_data)\n",
        "\n",
        "train_samples = int(split_ratio * len(full_train))\n",
        "valid_samples = len(full_train) - train_samples\n",
        "\n",
        "train_data, valid_data = random_split(full_train, [train_samples, valid_samples], generator=torch.Generator().manual_seed(SEED))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21eaf30b-8f6f-46c9-980e-980b0052bdf0",
      "metadata": {
        "id": "21eaf30b-8f6f-46c9-980e-980b0052bdf0"
      },
      "source": [
        "Again, we'll view how many examples are in each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3613c605-d4f9-4b61-b21a-b1baac19dad8",
      "metadata": {
        "id": "3613c605-d4f9-4b61-b21a-b1baac19dad8"
      },
      "outputs": [],
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e00d4e6-452e-4b10-bf51-7034db6ec41b",
      "metadata": {
        "id": "8e00d4e6-452e-4b10-bf51-7034db6ec41b"
      },
      "source": [
        "## Processing our data\n",
        "In the previous section we did some data exploration to understand our dataset's format. We noticed that each data sample consists of two strings: the sentiment label and the text.\n",
        "\n",
        "As we can't just feed strings into a recurrent neural network, we'll need to do some processing. Specifically:\n",
        "- We'll convert the **labels** into an integer (0 for negative, 1 for positive).\n",
        "- For the texts we'll:\n",
        "  1. Build a vocabulary.\n",
        "  2. Tokenize the text using SpaCy.\n",
        "  3. Convert each sentence into a vector of numerical vocabulary IDs.\n",
        "  4. Pad vectors to an equal length using padding tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifteen-spain",
      "metadata": {
        "id": "fifteen-spain"
      },
      "source": [
        "### Building a vocabulary\n",
        "In this section we'll build a _vocabulary_. This is a effectively a look up table where every unique word in your data set has a corresponding _index_ (an integer).\n",
        "\n",
        "We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
        "\n",
        "![](https://github.com/surrey-nlp/NLP-2025/blob/main/lab04/assets/sentiment5.png?raw=1)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one).\n",
        "\n",
        "There are two ways effectively cut down our vocabulary, we can either only take the top $n$ most common words or ignore words that appear less than $m$ times. We'll do the former, only keeping the top 25,000 words.\n",
        "\n",
        "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special _unknown_ or `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5edfd18b-37d6-48b2-b38c-370965d1bb13",
      "metadata": {
        "id": "5edfd18b-37d6-48b2-b38c-370965d1bb13"
      },
      "source": [
        "In order to build the vocab, however, we will need to tokenize each text. Let's define a tokenizer as a PyTorch module for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8219ab3-3c94-4150-ae2b-9f99ec50d2e6",
      "metadata": {
        "id": "c8219ab3-3c94-4150-ae2b-9f99ec50d2e6"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load spaCy English model once\n",
        "_nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
        "\n",
        "class SpacyTokenizer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, input):\n",
        "        def tok(text: str):\n",
        "            return [t.text.lower() for t in _nlp(text)]\n",
        "        if isinstance(input, list):\n",
        "            return [tok(text) for text in input]\n",
        "        if isinstance(input, str):\n",
        "            return tok(input)\n",
        "        raise ValueError(f\"Type {type(input)} is not supported.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf1da89-3c4a-4cab-9ebc-b1fbc42f5f0d",
      "metadata": {
        "id": "3cf1da89-3c4a-4cab-9ebc-b1fbc42f5f0d"
      },
      "source": [
        "## Tokenisation and Vocabulary Construction\n",
        "\n",
        "Our tokenizer is responsible for splitting raw text into individual tokens\n",
        "It can process either a single string or a batch of strings, which makes it flexible enough to use both when building the vocabulary and when preparing batches for training.\n",
        "\n",
        "Next, we construct a vocabulary directly from the training data. To do this, we:\n",
        "\n",
        "-Tokenise all training reviews\n",
        "\n",
        "-Count token frequencies using collections.Counter\n",
        "\n",
        "-Create a mapping from tokens to integer indices\n",
        "\n",
        "We only build the vocabulary from the training split to avoid leaking information from the test set.\n",
        "\n",
        "We also define two special tokens:\n",
        "\n",
        "`<pad>` â€” used to pad sequences to the same length within a batch\n",
        "\n",
        "`<unk>` â€” used to represent words that were not seen during training\n",
        "\n",
        "Any token not present in the vocabulary is automatically mapped to the <unk> index during numericalisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eleven-upgrade",
      "metadata": {
        "id": "eleven-upgrade"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "tokenizer = SpacyTokenizer()\n",
        "MAX_VOCAB_SIZE = 5000\n",
        "\n",
        "class SimpleVocab:\n",
        "    def __init__(self, itos):\n",
        "        self.itos = list(itos)\n",
        "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    def get_itos(self):\n",
        "        return self.itos\n",
        "    def get_stoi(self):\n",
        "        return self.stoi\n",
        "    def __getitem__(self, token):\n",
        "        # allow vocab[\"<pad>\"] style\n",
        "        return self.stoi.get(token, self.stoi[\"<unk>\"])\n",
        "\n",
        "# Build text vocab from the *training* split\n",
        "counter = Counter()\n",
        "for label, text in tqdm(train_data):\n",
        "    counter.update(tokenizer(text))\n",
        "\n",
        "specials = [\"<unk>\", \"<pad>\"]\n",
        "most_common = [tok for tok, _ in counter.most_common(MAX_VOCAB_SIZE - len(specials))]\n",
        "text_vocab = SimpleVocab(specials + most_common)\n",
        "\n",
        "# Label vocab (IMDB uses 0=neg, 1=pos)\n",
        "label_vocab = SimpleVocab([\"neg\", \"pos\"])\n",
        "\n",
        "print(f\"Unique tokens in text vocabulary: {len(text_vocab)}\")\n",
        "print(f\"Unique tokens in label vocabulary: {len(label_vocab)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dominican-handle",
      "metadata": {
        "id": "dominican-handle"
      },
      "source": [
        "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-alpha",
      "metadata": {
        "id": "final-alpha"
      },
      "source": [
        "We can see the vocabulary directly using either of the `get_stoi()` (**s**tring **to** **i**nt) or `get_itos()` (**i**nt **to**  **s**tring) methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "differential-evans",
      "metadata": {
        "id": "differential-evans"
      },
      "outputs": [],
      "source": [
        "text_vocab.get_itos()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impressive-married",
      "metadata": {
        "id": "impressive-married"
      },
      "source": [
        "Regarding the `<pad>` and `<unk>` tokens you can see at the front there, one of them is a padding token and the other is the unknown token.\n",
        "\n",
        "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded with the `<pad>` token.\n",
        "\n",
        "![](https://github.com/surrey-nlp/NLP-2025/blob/main/lab04/assets/sentiment6.png?raw=1)\n",
        "\n",
        "The unknown token will be used to replace any of the words encountered that are not part of our vocabulary. We can also check the labels, ensuring 0 is for negative and 1 is for positive.\n",
        "\n",
        "We can also check the labels, ensuring 0 is for negative and 1 is for positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "refined-secretariat",
      "metadata": {
        "id": "refined-secretariat"
      },
      "outputs": [],
      "source": [
        "label_vocab.get_stoi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z_2FxOfzH3wZ",
      "metadata": {
        "id": "z_2FxOfzH3wZ"
      },
      "outputs": [],
      "source": [
        "label_vocab.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "656b166f-6b61-40af-8298-f4ed17c66370",
      "metadata": {
        "id": "656b166f-6b61-40af-8298-f4ed17c66370"
      },
      "outputs": [],
      "source": [
        "# Top 20 most common tokens in the training data\n",
        "counter.most_common(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d156eaaa-61a4-4bbc-a76e-efa895e25925",
      "metadata": {
        "id": "d156eaaa-61a4-4bbc-a76e-efa895e25925"
      },
      "source": [
        "### Defining the rest of our data processing pipelines\n",
        "In the original version of this lab, torchtext transforms were used to build preprocessing pipelines.\n",
        "In this updated version, we keep the same *idea* (a repeatable preprocessing pipeline), but implement it\n",
        "using a small set of Python functions and a custom `collate_fn` inside the PyTorch `DataLoader`.\n",
        "\n",
        "**For each batch**, the `collate_fn`:\n",
        "1. Tokenises each review using the spaCy tokenizer\n",
        "2. Converts tokens to integer IDs using the vocabulary (numericalisation)\n",
        "3. Pads sequences in the batch to the same length using the `<pad>` token ID\n",
        "4. Converts labels to a tensor\n",
        "5. Returns the original sequence lengths (before padding)\n",
        "\n",
        "This means each batch contains:\n",
        "- `labels`: shape `[batch_size]`\n",
        "- `text`: shape `[batch_size, seq_len]`\n",
        "- `lengths`: shape `[batch_size]` (useful for packed sequences in the RNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c5821b-21d9-444f-84d4-2d0cadb5924e",
      "metadata": {
        "id": "24c5821b-21d9-444f-84d4-2d0cadb5924e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "PAD_IDX = text_vocab[\"<pad>\"]\n",
        "\n",
        "def numericalize(tokens):\n",
        "    return [text_vocab.stoi.get(tok, text_vocab[\"<unk>\"]) for tok in tokens]\n",
        "\n",
        "def text_transform(texts):\n",
        "    # texts: List[str]\n",
        "    token_lists = tokenizer(list(texts))\n",
        "    ids = [torch.tensor(numericalize(toks), dtype=torch.long) for toks in token_lists]\n",
        "    return pad_sequence(ids, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "def lengths_transform(texts):\n",
        "    token_lists = tokenizer(list(texts))\n",
        "    return torch.tensor([len(toks) for toks in token_lists], dtype=torch.long)\n",
        "\n",
        "def label_transform(labels):\n",
        "    # labels: List[int] where 0=neg, 1=pos\n",
        "    return torch.tensor(labels, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbfd346-5361-4e3b-900b-6f2a02459e18",
      "metadata": {
        "id": "2cbfd346-5361-4e3b-900b-6f2a02459e18"
      },
      "source": [
        "We'll also define an additional transform for the texts that will transform each text into its length *after* it gets tokenized but *before* it gets padded. These lengths will be useful when we *pack the padded sequences* later.\n",
        "\n",
        "Note that applying `text_transform` and `lengths_transform` to our texts will mean that we will tokenize them twice, which is somewhat inefficient, but we will do so anyway for the sake of simplicity. In your implementations, you might want to consider having a \"shared\" pipeline that handles tokenization / vocabulary transformation and then having two separate pipelines that do further processing over that (one to extract the lengths and one to pad each tensor, for example)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "042c31fb-df56-41b5-bee0-ea4e7c05a088",
      "metadata": {
        "id": "042c31fb-df56-41b5-bee0-ea4e7c05a088"
      },
      "source": [
        "### Understanding the processing being done\n",
        "Before moving on, let's examine what exactly each step in our processing pipelines will do to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d80b8a3-3a64-46a9-a8dd-bf909b799f18",
      "metadata": {
        "id": "0d80b8a3-3a64-46a9-a8dd-bf909b799f18"
      },
      "outputs": [],
      "source": [
        "sample_label, sample_text = train_data[0]\n",
        "mapping = {0: \"neg\", 1: \"pos\"}\n",
        "\n",
        "print(f\"Text before any processing: {sample_text[:200]}...\")\n",
        "print(f\"Label before any processing: {sample_label} ({mapping[sample_label]})\\n\")\n",
        "\n",
        "# Text Processing Pipeline\n",
        "sample_tokens = tokenizer(sample_text)\n",
        "print(f\"Text after Tokenizer (first 30 tokens): {sample_tokens[:30]}\\n\")\n",
        "\n",
        "sample_ids = numericalize(sample_tokens)\n",
        "print(f\"Text after Numericalisation (first 30 ids): {sample_ids[:30]}\\n\")\n",
        "\n",
        "sample_tensor = torch.tensor(sample_ids, dtype=torch.long)\n",
        "print(f\"Text as tensor (shape): {sample_tensor.shape}\\n\")\n",
        "\n",
        "# Label Processing Pipeline\n",
        "print(f\"Label after label transform: {label_transform([sample_label])}\\n\")\n",
        "\n",
        "# Length Processing Pipeline\n",
        "print(f\"Text length after tokenization: {lengths_transform([sample_text]).item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "421153cd-82e3-4c67-94d0-4dfcf0ef0f21",
      "metadata": {
        "id": "421153cd-82e3-4c67-94d0-4dfcf0ef0f21"
      },
      "source": [
        "Do note that when we take two texts of differing sizes, they will be padded after going through the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b98820f-4104-41c7-86ca-0bed7ae55429",
      "metadata": {
        "id": "5b98820f-4104-41c7-86ca-0bed7ae55429"
      },
      "outputs": [],
      "source": [
        "(lab0_label, lab0_text) = train_data[0]\n",
        "(lab1_label, lab1_text) = train_data[1]\n",
        "\n",
        "processed_sample_texts = text_transform([lab0_text, lab1_text])\n",
        "lengths = lengths_transform([lab0_text, lab1_text])\n",
        "diff = abs(lengths[0].item() - lengths[1].item()) + 5\n",
        "\n",
        "print(f\"Padding vocabulary index: {PAD_IDX}\")\n",
        "print(\"Respective text lengths after tokenization: \", lengths.tolist())\n",
        "print(\"Tensor shape after text processing (padded): \", tuple(processed_sample_texts.shape))\n",
        "print(f\"Last {diff} ids of text 0 after processing:\\n\", processed_sample_texts[0][-diff:])\n",
        "print(f\"Last {diff} ids of text 1 after processing:\\n\", processed_sample_texts[1][-diff:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8f720e8-4c50-4c1c-970e-92e1e6f3567f",
      "metadata": {
        "id": "f8f720e8-4c50-4c1c-970e-92e1e6f3567f"
      },
      "source": [
        "Notice how one text is shorter, but after the `ToTensor` transformation both texts end up with the same sentence length, and how the shorter text was padded with the padding token from the vocabulary for its missing characters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a3e175-dfe5-44b0-8b07-03fa1a78473d",
      "metadata": {
        "id": "d5a3e175-dfe5-44b0-8b07-03fa1a78473d"
      },
      "source": [
        "### The DataLoader\n",
        "Now we've got all our originally planned processing set up and ready to go. The final step is to put our data into a PyTorch `DataLoader`. The `DataLoader` will help us iterate over the data in batches ofexamples at each iteartion.\n",
        "\n",
        "PyTorch's `DataLoader` provides quite a few features to let us iterate over data, but it doesn't do any processing on its own. We need to define a `collate_batch` function where we explicitly define what processing steps each batch of data will need to go through when it goes through the data loader. In essence, we'll just use that function to feed our data through the pipelines we created previously.\n",
        "\n",
        "We also want to place the tensors returned on the GPU (if you're using one). PyTorch handles this using `torch.device`, which we can then pass our tensors to in `collate_batch`. Finally, note that we convert the labels to float values so we can compute loss later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certified-hours",
      "metadata": {
        "id": "certified-hours"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def collate_batch(batch):\n",
        "    labels, texts = zip(*batch)\n",
        "    lengths = lengths_transform(list(texts))\n",
        "    texts = text_transform(list(texts))\n",
        "    labels = label_transform(list(labels))\n",
        "    return labels.float().to(DEVICE), texts.to(DEVICE), lengths.cpu()\n",
        "\n",
        "def _get_dataloader(data):\n",
        "    return DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "train_dataloader = _get_dataloader(train_data)\n",
        "valid_dataloader = _get_dataloader(valid_data)\n",
        "test_dataloader = _get_dataloader(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468b1e41-2697-46f2-af10-067389ed684b",
      "metadata": {
        "id": "468b1e41-2697-46f2-af10-067389ed684b",
        "tags": []
      },
      "source": [
        "### Summary\n",
        "- We imported the `IMDB` dataset from PyTorch and converted it from an iterable-style data stream data set to a map-style data set with `to_map_style_dataset`.\n",
        "- We split it further to obtain a validation set using `random_split`.\n",
        "- We defined a Tokenizer using SpaCy as a PyTorch Module.\n",
        "- We used `a custom vocabulary built from the training data` and some auxiliary functions to create our vocabulary.\n",
        "- We used `PyTorch.transforms` to define processing pipelines.\n",
        "- We used `torch.utils.data.DataLoader` as well as an auxiliary function to put our data through our data pipelines and into batches.\n",
        "\n",
        "That was a lot, but we are *finally* ready to put our data through our model!\n",
        "\n",
        "Unfortunately, PyTorch is still a very young project and a lot of reference material still uses its old legacy API. When trying to look deeper into PyTorch, it's recommended to make sure you're checking its [latest documentation](https://pytorch.org/text/stable/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aggregate-stress",
      "metadata": {
        "id": "aggregate-stress",
        "tags": []
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "The next stage is building the model that we'll eventually train and evaluate.\n",
        "\n",
        "There is a small amount of boilerplate code when creating models in PyTorch, note how our `RNN` class is a sub-class of `nn.Module` and the use of `super`.\n",
        "\n",
        "Within the `__init__` we define the _layers_ of the module. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
        "\n",
        "The embedding layer is used to transform our sparse word representations (sparse as most of the elements are 0) into dense embedding vectors (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
        "\n",
        "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
        "\n",
        "![](https://github.com/surrey-nlp/NLP-2025/blob/main/lab04/assets/sentiment7.png?raw=1)\n",
        "\n",
        "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
        "\n",
        "The `forward` method is called when we feed examples into our model.\n",
        "\n",
        "Each batch, `texts`, is a tensor of size **[batch_size, batch_sentence_length]**. That is a batch of sentences, each having each word converted into its vocabulary index in our previous processing steps. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*. The `Embedding` layer will take care of converting these indices into one-hot vectors. Note that `batch_sentence_length` is the length of the largest sentence in the batch.\n",
        "\n",
        "The input batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. `embedded` is a tensor of size **[batch size, batch_sentence_length, embedding dim]**.\n",
        "\n",
        "Each batch, we also have access to `lengths` which is a tensor of the lengths of each sentence before it was padded to be of size `batch_sentence_length`. We will use these lengths to essentially remove the padding present in each sentence into the batch, and combine all sentences into one large vector that will get fed through the RNN. This is mainly a performance optimization but it is generally good practice. Note that even if we are combining all our input into one continuous tensor, the RNN will still output results for individual sentences, the main thing we gain is that we don't waste computation time processing the padding present in the original tensors.\n",
        "\n",
        "`embedded` is then fed into the RNN. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
        "\n",
        "The RNN returns 2 tensors, `output` normally of size **[batch size, batch_sentence length, hidden dim]** and `hidden` of size **[1, batch size, hidden dim]**. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. Since we're using packed padded sequences, `output` will also be a packed padded sequence, so its size will be slightly different and overall smaller the normal size.\n",
        "\n",
        "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction. Note the `squeeze` method, which is used to remove a dimension of size 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "divided-publicity",
      "metadata": {
        "id": "divided-publicity"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, texts, lengths):\n",
        "        embedded = self.embedding(texts)                          # VV note that lengths need to be on the CPU\n",
        "        embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        return self.fc(hidden.squeeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "american-village",
      "metadata": {
        "id": "american-village"
      },
      "source": [
        "We now create an instance of our RNN class.\n",
        "\n",
        "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size.\n",
        "\n",
        "The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
        "\n",
        "The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
        "\n",
        "The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "danish-guest",
      "metadata": {
        "id": "danish-guest"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(text_vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unique-exemption",
      "metadata": {
        "id": "unique-exemption"
      },
      "source": [
        "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valued-growth",
      "metadata": {
        "id": "valued-growth"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unavailable-convenience",
      "metadata": {
        "id": "unavailable-convenience"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "familiar-contamination",
      "metadata": {
        "id": "familiar-contamination"
      },
      "source": [
        "Now we'll set up the training and then train the model.\n",
        "\n",
        "First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use _stochastic gradient descent_ (SGD). The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "graphic-justice",
      "metadata": {
        "id": "graphic-justice"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crude-memorabilia",
      "metadata": {
        "id": "crude-memorabilia"
      },
      "source": [
        "Next, we'll define our loss function. In PyTorch this is commonly called a criterion.\n",
        "\n",
        "The loss function here is _binary cross entropy with logits_.\n",
        "\n",
        "Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the _sigmoid_ or _logit_ functions.\n",
        "\n",
        "We then use this this bound scalar to calculate the loss using binary cross entropy.\n",
        "\n",
        "The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "photographic-difficulty",
      "metadata": {
        "id": "photographic-difficulty"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "enormous-norman",
      "metadata": {
        "id": "enormous-norman"
      },
      "source": [
        "Using `.to`, we can place the model and the criterion on the GPU (if we have one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "authentic-change",
      "metadata": {
        "id": "authentic-change"
      },
      "outputs": [],
      "source": [
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "strategic-massachusetts",
      "metadata": {
        "id": "strategic-massachusetts"
      },
      "source": [
        "Our criterion function calculates the loss, however we have to write our function to calculate the accuracy.\n",
        "\n",
        "This function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, we then round them to the nearest integer. This rounds any value greater than 0.5 to 1 (a positive sentiment) and the rest to 0 (a negative sentiment).\n",
        "\n",
        "We then calculate how many rounded predictions equal the actual labels and average it across the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "domestic-smell",
      "metadata": {
        "id": "domestic-smell"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "geographic-population",
      "metadata": {
        "id": "geographic-population"
      },
      "source": [
        "The `train` function iterates over all examples, one batch at a time.\n",
        "\n",
        "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
        "\n",
        "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "We then feed the batch of sentences and their original lengths, `texts` and `lengths` accordingly, into the model. Note, you do not need to do `model.forward(texts, lengths)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
        "\n",
        "The loss and accuracy are then calculated using our predictions and the labels, `labels`, with the loss being averaged over all examples in the batch.\n",
        "\n",
        "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
        "\n",
        "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value.\n",
        "\n",
        "Finally, we return the loss and accuracy, averaged across the epoch. The `len` of an iterator is the number of batches in the iterator.\n",
        "\n",
        "You may recall that we converted the labels to float in `collate_batch()`. This is because `ToTensor` sets tensors to be `LongTensor`s by default, however our criterion expects both inputs to be `FloatTensor`s. The alternative method of doing this would be to do the conversion inside the `train` function by passing `labels.float()` instad of `labels` to the criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "meaningful-order",
      "metadata": {
        "id": "meaningful-order"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator, desc=\"\\tTraining\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        labels, texts, lengths = batch  # Note that this has to match the order in collate_batch\n",
        "        predictions = model(texts, lengths).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "overall-wagon",
      "metadata": {
        "id": "overall-wagon"
      },
      "source": [
        "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
        "\n",
        "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
        "\n",
        "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
        "\n",
        "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "economic-ultimate",
      "metadata": {
        "id": "economic-ultimate"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator, desc=\"\\tEvaluation\"):\n",
        "            labels, texts, lengths = batch  # Note that this has to match the order in collate_batch\n",
        "            predictions = model(texts, lengths).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "immediate-arena",
      "metadata": {
        "id": "immediate-arena"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assisted-field",
      "metadata": {
        "id": "assisted-field"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "similar-buffer",
      "metadata": {
        "id": "similar-buffer"
      },
      "source": [
        "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets.\n",
        "\n",
        "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proper-season",
      "metadata": {
        "id": "proper-season"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'} for training.\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "\n",
        "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-making",
      "metadata": {
        "id": "saved-making"
      },
      "source": [
        "You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook.\n",
        "\n",
        "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weekly-mailing",
      "metadata": {
        "collapsed": true,
        "id": "weekly-mailing"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "funky-albany",
      "metadata": {
        "id": "funky-albany"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "In the next notebook, the improvements we will make are:\n",
        "- pre-trained word embeddings\n",
        "- different RNN architectures\n",
        "- bidirectional RNN\n",
        "- multi-layer RNN\n",
        "- regularization\n",
        "- a different optimizer\n",
        "\n",
        "This will allow us to achieve ~84% accuracy."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_lab_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
