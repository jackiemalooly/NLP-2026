{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lab 02: POS Tagging, Morphology, Lemmatization, Dependency Parsing, and Tokenization\n",
    "\n",
    "## Introduction  \n",
    "In this tutorial, we will explore some core NLP tasks using **spaCy**, a powerful and efficient Python library for NLP. Additionally, we will examine tokenization techniques used in modern language models.  \n",
    "\n",
    "### Topics Covered:  \n",
    "\n",
    "1. **Tokenization**  \n",
    "2. **Part-of-Speech (POS) Tagging**  \n",
    "3. **Lemmatization**  \n",
    "4. **Morphology**  \n",
    "5. **Dependency Parsing**  \n",
    "6. **Subword Tokenization**  \n",
    "\n",
    "## Prerequisites  \n",
    "\n",
    "Before we begin, ensure that you have **spaCy** installed in your environment. If you are using the `NLP2025` environment, make sure it is activated. You can install **spaCy** using the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab02/lab02_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "colab_button = HTML(\n",
    "    '<a href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab02/lab02_Tokenization.ipynb\" target=\"_parent\">'\n",
    "    '<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>'\n",
    ")\n",
    "display(colab_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (0.21.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (4.67.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (82.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from spacy) (26.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from jinja2->spacy) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the spaCy model for the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing spaCy\n",
    "Let's start by importing the spaCy library and loading the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning, let's define a couple example strings that we can look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = \"Cats like to chase mice.\"\n",
    "long_text = \"The University of Surrey is a U.K. university founded in 1966, with a budget of £314.0 million.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization  \n",
    "\n",
    "**Tokenization** is the process of breaking down text input into smaller units called **tokens**, which can be **words, punctuation marks, or other meaningful elements**. This is a fundamental step in NLP as it enables structured text analysis.  \n",
    "\n",
    "### Tokenization in spaCy  \n",
    "\n",
    "In **spaCy**, tokenization is performed using language-specific grammatical rules. For example:  \n",
    "- Punctuation at the end of a sentence is **split off** as a separate token.  \n",
    "- Abbreviations like **\"U.K.\"** retain their periods within a single token.  \n",
    "\n",
    "### How spaCy Handles Tokenization  \n",
    "\n",
    "- The **input** to the tokenizer is a **Unicode text**.  \n",
    "- The **output** is a **Doc object**, which consists of individual tokens.  \n",
    "- We can **iterate** over tokens and access attributes such as `token.text`.  \n",
    "- spaCy's tokenizer is **non-destructive**, meaning it preserves the original text while providing structured access to tokens.  \n",
    "\n",
    "This efficient tokenization process enables deeper linguistic analysis while maintaining the integrity of the original text.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text\n",
      "----------  ------\n",
      "         1  Cats\n",
      "         2  like\n",
      "         3  to\n",
      "         4  chase\n",
      "         5  mice\n",
      "         6  .\n"
     ]
    }
   ],
   "source": [
    "# Useful Library for formatting the table\n",
    "import tabulate\n",
    "\n",
    "doc = nlp(short_text)\n",
    "\n",
    "# Plot table \n",
    "table = []\n",
    "for count, token in enumerate(doc):\n",
    "    table.append([count + 1, token.text])\n",
    "\n",
    "print(tabulate.tabulate(table, headers=['Position','Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text\n",
      "----------  ----------\n",
      "         1  The\n",
      "         2  University\n",
      "         3  of\n",
      "         4  Surrey\n",
      "         5  is\n",
      "         6  a\n",
      "         7  U.K.\n",
      "         8  university\n",
      "         9  founded\n",
      "        10  in\n",
      "        11  1966\n",
      "        12  ,\n",
      "        13  with\n",
      "        14  a\n",
      "        15  budget\n",
      "        16  of\n",
      "        17  £\n",
      "        18  314.0\n",
      "        19  million\n",
      "        20  .\n"
     ]
    }
   ],
   "source": [
    "# Again for our longer text\n",
    "doc2 = nlp(long_text)\n",
    "\n",
    "table = []\n",
    "for count, token in enumerate(doc2):\n",
    "    table.append([count + 1, token.text])\n",
    "\n",
    "print(tabulate.tabulate(table, headers=['Position','Text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-Speech (POS) Tagging  \n",
    "\n",
    "**Part-of-Speech (POS) tagging** is the process of assigning grammatical tags to individual words in a sentence, indicating their role, such as **noun, verb, adjective,** etc. This helps in understanding the **syntactic structure** of a sentence and is fundamental in many NLP tasks.  \n",
    "\n",
    "### Using spaCy for POS Tagging  \n",
    "\n",
    "Since we have previously processed the text input using **spaCy**, we can easily retrieve the POS tag for each token with a simple attribute call `token.pos_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text    POS Tag\n",
      "----------  ------  ---------\n",
      "         1  Cats    NOUN\n",
      "         2  like    VERB\n",
      "         3  to      PART\n",
      "         4  chase   VERB\n",
      "         5  mice    NOUN\n",
      "         6  .       PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For our short sentence\n",
    "POS_Tags = []\n",
    "for count, token in enumerate(doc):\n",
    "    POS_Tags.append([count + 1, token.text, token.pos_])\n",
    "\n",
    "print(tabulate.tabulate(POS_Tags, headers=['Position','Text', 'POS Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        POS Tag\n",
      "----------  ----------  ---------\n",
      "         1  The         DET\n",
      "         2  University  PROPN\n",
      "         3  of          ADP\n",
      "         4  Surrey      PROPN\n",
      "         5  is          AUX\n",
      "         6  a           DET\n",
      "         7  U.K.        PROPN\n",
      "         8  university  NOUN\n",
      "         9  founded     VERB\n",
      "        10  in          ADP\n",
      "        11  1966        NUM\n",
      "        12  ,           PUNCT\n",
      "        13  with        ADP\n",
      "        14  a           DET\n",
      "        15  budget      NOUN\n",
      "        16  of          ADP\n",
      "        17  £           SYM\n",
      "        18  314.0       NUM\n",
      "        19  million     NUM\n",
      "        20  .           PUNCT\n"
     ]
    }
   ],
   "source": [
    "# And our longer sentence\n",
    "POS_Tags = []\n",
    "for count, token in enumerate(doc2):\n",
    "    POS_Tags.append([count + 1, token.text, token.pos_])\n",
    "\n",
    "print(tabulate.tabulate(POS_Tags, headers=['Position','Text', 'POS Tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the example above, we can see several **POS Tags**. Some common examples include:\n",
    "\n",
    "- **DET**: Determiner  \n",
    "- **PROPN**: Proper Noun  \n",
    "- **ADP**: Adposition  \n",
    "\n",
    "These tags represent different parts of speech in a sentence and are crucial for understanding the syntactic structure of the language.\n",
    "\n",
    "### Why Use POS Tagging?\n",
    "\n",
    "In NLP, understanding the grammatical structure of sentences can be extremely valuable for many tasks. POS tagging helps computers to identify the roles that different words play within a sentence, such as subjects, objects, or actions.\n",
    "\n",
    "However, in some tasks, it may also be useful to **discard certain words** based on their POS tags. For example:\n",
    "\n",
    "- **Sentiment Analysis**:  \n",
    "  In sentiment analysis, words like **articles** (e.g., *\"the\"*, *\"a\"*) and **pronouns** (e.g., *\"he\"*, *\"she\"*) might be discarded because they contribute little to the overall sentiment of the text.\n",
    "\n",
    "By filtering out less relevant POS tags, the model can focus on words that carry more meaning and help improve task performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization  \n",
    "\n",
    "**Lemmatization** is the process of reducing words to their **base** or **root** form, known as a **lemma**. This helps in **text normalization** by converting different inflectional forms of a word into a single standardized form.  \n",
    "\n",
    "Lemmatization is particularly useful in NLP tasks such as:  \n",
    "- Improving text **search and retrieval**  \n",
    "- Enhancing **sentiment analysis**  \n",
    "- Reducing **dimensionality** in text-based models  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text    Lemma\n",
      "----------  ------  -------\n",
      "         1  Cats    cat\n",
      "         2  like    like\n",
      "         3  to      to\n",
      "         4  chase   chase\n",
      "         5  mice    mouse\n",
      "         6  .       .\n"
     ]
    }
   ],
   "source": [
    "# For our short sentence\n",
    "Morphs = []\n",
    "for count, token in enumerate(doc):\n",
    "    Morphs.append([count + 1, token.text, token.lemma_])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text','Lemma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        Lemma\n",
      "----------  ----------  ----------\n",
      "         1  The         the\n",
      "         2  University  University\n",
      "         3  of          of\n",
      "         4  Surrey      Surrey\n",
      "         5  is          be\n",
      "         6  a           a\n",
      "         7  U.K.        U.K.\n",
      "         8  university  university\n",
      "         9  founded     found\n",
      "        10  in          in\n",
      "        11  1966        1966\n",
      "        12  ,           ,\n",
      "        13  with        with\n",
      "        14  a           a\n",
      "        15  budget      budget\n",
      "        16  of          of\n",
      "        17  £           £\n",
      "        18  314.0       314.0\n",
      "        19  million     million\n",
      "        20  .           .\n"
     ]
    }
   ],
   "source": [
    "# And our longer sentence\n",
    "Morphs = []\n",
    "for count, token in enumerate(doc2):\n",
    "    Morphs.append([count + 1, token.text, token.lemma_])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text','Lemma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Morphology  \n",
    "\n",
    "**Morphology** is the study of the structure of words and their components, such as **prefixes, suffixes,** and **roots**. In essence, it is the process through which the root form (lemma) of a word is modified by the addition of prefixes or suffixes, altering its meaning or grammatical function.  \n",
    "\n",
    "In **spaCy**, we can access detailed morphological information for each token, which includes features such as:  \n",
    "- **Number** (singular or plural)  \n",
    "- **Tense** (present, past, etc.)  \n",
    "- **Mood**: Indicates the mode or manner in which the action is expressed (e.g., **indicative**, **imperative**, or **subjunctive**).  \n",
    "  - Example: *\"She eats\"* (indicative) vs. *\"Eat!\"* (imperative)\n",
    "- **Aspect**: Describes the temporal flow or completion of an action (e.g., **perfective**, **progressive**, or **habitual**).  \n",
    "  - Example: *\"I am eating\"* (progressive) vs. *\"I have eaten\"* (perfective)  \n",
    "\n",
    "This morphological analysis is essential for understanding how words relate to one another in context and is crucial for tasks such as syntactic parsing and word generation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text    Morphology\n",
      "----------  ------  -----------------------\n",
      "         1  Cats    Number=Plur\n",
      "         2  like    Tense=Pres|VerbForm=Fin\n",
      "         3  to\n",
      "         4  chase   VerbForm=Inf\n",
      "         5  mice    Number=Plur\n",
      "         6  .       PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "# For our short sentence\n",
    "Morphs = []\n",
    "for count, token in enumerate(doc):\n",
    "    Morphs.append([count + 1, token.text, token.morph])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text', 'Morphology']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text        Morphology\n",
      "----------  ----------  -----------------------------------------------------\n",
      "         1  The         Definite=Def|PronType=Art\n",
      "         2  University  Number=Sing\n",
      "         3  of\n",
      "         4  Surrey      Number=Sing\n",
      "         5  is          Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "         6  a           Definite=Ind|PronType=Art\n",
      "         7  U.K.        Number=Sing\n",
      "         8  university  Number=Sing\n",
      "         9  founded     Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "        10  in\n",
      "        11  1966        NumType=Card\n",
      "        12  ,           PunctType=Comm\n",
      "        13  with\n",
      "        14  a           Definite=Ind|PronType=Art\n",
      "        15  budget      Number=Sing\n",
      "        16  of\n",
      "        17  £\n",
      "        18  314.0       NumType=Card\n",
      "        19  million     NumType=Card\n",
      "        20  .           PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "# And our longer sentence\n",
    "Morphs = []\n",
    "for count, token in enumerate(doc2):\n",
    "    Morphs.append([count + 1, token.text, token.morph])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text', 'Morphology']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dependency Parsing  \n",
    "\n",
    "Dependency parsing involves analyzing the grammatical structure of a sentence and establishing relationships between **head** words and their **modifiers**. This technique allows us to decompose a sentence into multiple sections, assuming a direct connection between each linguistic unit. These relationships are typically represented as a **tree structure**, illustrating how words depend on one another.  \n",
    "\n",
    "### Example  \n",
    "\n",
    "**Sentence:**  \n",
    "*\"I prefer the morning flight through Denver.\"*  \n",
    "\n",
    "The diagram below visualizes the sentence's dependency structure:  \n",
    "\n",
    "![Dependency Parsing](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/29920Screenshot-127.webp)  \n",
    "*[Source](https://www.analyticsvidhya.com/blog/2021/12/dependency-parsing-in-natural-language-processing-with-examples/)*  \n",
    "\n",
    "### Understanding the Dependency Structure  \n",
    "\n",
    "In the diagram:  \n",
    "\n",
    "- **Directed arcs** illustrate grammatical relationships between words in the sentence.  \n",
    "- The **root** of the tree, *prefer*, serves as the central unit of the sentence.  \n",
    "- Each dependency is labeled with a **dependency tag**, which specifies the relationship between two words.  \n",
    "\n",
    "For instance, in the phrase **\"flight to Denver\"**, the noun *Denver* modifies the meaning of *flight*. This creates a **dependency** where:  \n",
    "\n",
    "- *Flight* is the **head** (governing word).  \n",
    "- *Denver* is the **dependent** (child node).  \n",
    "- This relationship is marked by the **nmod** (nominal modifier) tag, indicating that *Denver* provides additional information about *flight*.  \n",
    "\n",
    "Dependency parsing plays a crucial role in natural language processing (NLP), helping models understand syntactic structures and improving tasks such as named entity recognition, question answering, and machine translation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this can be done easily with spaCy through the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position  Text    Dependency    Children\n",
      "----------  ------  ------------  ----------------------\n",
      "         1  Cats    nsubj         []\n",
      "         2  like    ROOT          ['Cats', 'chase', '.']\n",
      "         3  to      aux           []\n",
      "         4  chase   xcomp         ['to', 'mice']\n",
      "         5  mice    dobj          []\n",
      "         6  .       punct         []\n"
     ]
    }
   ],
   "source": [
    "Dependenct_Parsing = []\n",
    "for count, token in enumerate(doc):\n",
    "    Dependenct_Parsing.append([count + 1, token.text, token.dep_, [child.text for child in token.children]])\n",
    "\n",
    "print(tabulate.tabulate(Dependenct_Parsing, headers=['Position','Text', 'Dependency', 'Children']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table might look very confusing, which is why spaCy offers a quick way to easily view the tree structure with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"37907e9493154f0ebfbd38d5f1941d43-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Cats</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">like</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">chase</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">mice.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-37907e9493154f0ebfbd38d5f1941d43-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-37907e9493154f0ebfbd38d5f1941d43-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-37907e9493154f0ebfbd38d5f1941d43-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-37907e9493154f0ebfbd38d5f1941d43-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-37907e9493154f0ebfbd38d5f1941d43-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-37907e9493154f0ebfbd38d5f1941d43-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-37907e9493154f0ebfbd38d5f1941d43-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-37907e9493154f0ebfbd38d5f1941d43-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"fff408a6818a494ab05cb700a22dc396-0\" class=\"displacy\" width=\"3200\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">University</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Surrey</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">founded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">1966,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">budget</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">£</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">314.0</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-2\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-6\" stroke-width=\"2px\" d=\"M770,352.0 C770,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,354.0 L1618.0,342.0 1602.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1785.0,354.0 L1793.0,342.0 1777.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-10\" stroke-width=\"2px\" d=\"M770,352.0 C770,2.0 1975.0,2.0 1975.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,354.0 L1983.0,342.0 1967.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-11\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,354.0 L2162,342.0 2178,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-12\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,177.0 2315.0,177.0 2315.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2315.0,354.0 L2323.0,342.0 2307.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-13\" stroke-width=\"2px\" d=\"M2345,352.0 C2345,264.5 2485.0,264.5 2485.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2485.0,354.0 L2493.0,342.0 2477.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-14\" stroke-width=\"2px\" d=\"M2695,352.0 C2695,177.0 3015.0,177.0 3015.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,354.0 L2687,342.0 2703,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-15\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,264.5 3010.0,264.5 3010.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,354.0 L2862,342.0 2878,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-fff408a6818a494ab05cb700a22dc396-0-16\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,89.5 3020.0,89.5 3020.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-fff408a6818a494ab05cb700a22dc396-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3020.0,354.0 L3028.0,342.0 3012.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And a more complicated one\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc2, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subword Tokenization  \n",
    "\n",
    "Tokenization is the process of breaking down a sentence into smaller units, enabling AI models to process text as discrete tokens rather than as a continuous block of text. In previous sections, you have used spaCy for tokenization, which primarily segments text into individual words. While this approach is efficient, it struggles with handling uncommon or out-of-vocabulary (OOV) words.  \n",
    "\n",
    "To address this limitation, modern tokenization techniques predominantly use **subword-based methods**. Instead of strictly segmenting text into words, these approaches break words into smaller subword units when necessary. For example, the word *unhappiness* might be tokenized into *un* and *happiness*. This strategy offers several advantages:  \n",
    "\n",
    "- **Improved Handling of Rare Words** – By decomposing words into meaningful subunits, the model can recognize and generate words that were not explicitly seen during training.  \n",
    "- **Compact Vocabulary** – Instead of storing an extensive vocabulary of all possible words, subword tokenization relies on a smaller set of subunits, which can be combined to form complex words.  \n",
    "- **Efficient Representation** – By balancing whole-word tokens with subword segments, this method optimizes both memory usage and model performance.  \n",
    "\n",
    "(**Note**: Often tokenizers try to maintain words that are frequently used, and split rare words into smaller subwords)\n",
    "\n",
    "We will therefore explore three subword tokenization techniques:  \n",
    "\n",
    "1. **WordPiece**  \n",
    "2. **Byte-Pair Encoding (BPE)**  \n",
    "3. **SentencePiece**  \n",
    "\n",
    "These tokenization methods have become standard in modern NLP models and are widely used in recent Large Language Models (LLMs).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's define a simple string that we will be tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is incontrovertibly a good module.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. WordPiece Tokenization\n",
    "\n",
    "**WordPiece Tokenization** is a subword tokenization technique used in models like BERT (Bidirectional Encoder Representations from Transformers). It breaks down words into subwords, that can efficiently handle complex words, unknown terms, or out-of-vocabulary (OOV) words.\n",
    "\n",
    "WordPiece works by iteratively merging the most frequent pairs of characters or subword units in a large corpus. The resulting subwords represent the language's most frequent word components, which helps to reduce the size of the vocabulary while maintaining full language coverage.\n",
    "\n",
    "### Example: Tokenizing Text with BERT’s WordPiece Tokenizer\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to showcase how the BERT tokenizer works. We’ll tokenize a sample sentence, convert the tokens into token IDs, and then decode those IDs back into a human-readable string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: colorama in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (from typer-slim->transformers) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "# First, install the required library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Tokens: ['natural', 'language', 'processing', 'is', 'inc', '##ont', '##rove', '##rti', '##bly', 'a', 'good', 'module', '.']\n",
      "\n",
      "BERT Token IDs: [3019, 2653, 6364, 2003, 4297, 12162, 17597, 28228, 6321, 1037, 2204, 11336, 1012]\n",
      "\n",
      "Decoded Text: natural language processing is incontrovertibly a good module.\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary module from Hugging Face transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Step 1: Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nBERT Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nBERT Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the word **\"incontrovertibly\"**, which is quite rare, is split into **5 subwords** by the tokenizer. This process of splitting words into smaller subunits is particularly useful for handling rare or out-of-vocabulary (OOV) words.\n",
    "\n",
    "Each subword is represented as a **token**, and you can see that certain tokens are prefixed with `##`. This notation indicates that these subwords are continuations of a previous subword (i.e., they are not starting a new token). The tokenizer has broken down the word into smaller, more frequent subwords that are part of the model’s vocabulary.\n",
    "\n",
    "### Why Do We Use Token IDs?\n",
    "\n",
    "As shown above, the tokens are also associated with **token IDs**. These token IDs are numerical representations of the words or subwords. In the context of machine learning and NLP models, it's crucial to convert words into numbers because models operate on numerical data.\n",
    "\n",
    "Each token is mapped to a unique ID in the model’s vocabulary, which allows the model to process text efficiently. This conversion is essential because:\n",
    "\n",
    "- **Models can't understand raw text**: Machine learning models, including NLP models, don't process text directly. Instead, they process **numerical representations** of words.\n",
    "- **Token IDs map to model parameters**: The model's vocabulary is essentially a map of tokens (words or subwords) to unique IDs. These IDs are used by the model to look up the corresponding word embeddings (vector representations) in the model’s parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. SentencePiece Tokenization\n",
    "\n",
    "**SentencePiece** is another popular subword tokenization technique used in models like T5 (Text-to-Text Transfer Transformer) and other transformer-based architectures.\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to demonstrate how the **SentencePiece tokenizer** works. We will also be training our own SentencePiece tokenizer later on!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SentencePiece Tokens: ['▁Natural', '▁Language', '▁Processing', '▁is', '▁in', 'contro', 'vert', 'ibly', '▁', 'a', '▁good', '▁module', '.']\n",
      "\n",
      "SentencePiece Token IDs: [6869, 10509, 19125, 19, 16, 23862, 3027, 15596, 3, 9, 207, 6008, 5]\n",
      "\n",
      "Decoded Text: Natural Language Processing is incontrovertibly a good module.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Step 1: Load a pre-trained SentencePiece tokenizer (T5 model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens using SentencePiece\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nSentencePiece Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nSentencePiece Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Byte-Pair Encoding (BPE) Tokenization\n",
    "\n",
    "**Byte-Pair Encoding (BPE)** is also another subword tokenization technique used in models like GPT (Generative Pretrained Transformer) and other transformer-based architectures. \n",
    "\n",
    "### Byte-Level BPE Tokenization\n",
    "\n",
    "Instead of treating text as sequences of **Unicode characters** (such as 'a', 'b', 'c', etc.), **byte-level BPE** tokenizes text at the **byte level**. Each character, word, and symbol is first converted into its corresponding **byte representation**.\n",
    "\n",
    "The **base vocabulary** for byte-level BPE is much smaller, consisting of only **256 byte values**, as there are 256 possible byte values. This ensures that any character can be represented without needing to resort to an **unknown token** for out-of-vocabulary (OOV) words.\n",
    "\n",
    "This approach allows models like **GPT-2** and **RoBERTa** to handle any character or symbol, including those from different languages, special symbols, or rare characters, without needing additional vocabularies or dealing with OOV issues.\n",
    "\n",
    "\n",
    "### Example: Tokenizing Text with BPE Tokenizer\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to demonstrate how a BPE tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BPE Tokens: ['Natural', 'ĠLanguage', 'ĠProcessing', 'Ġis', 'Ġinc', 'ont', 'ro', 'vert', 'ibly', 'Ġa', 'Ġgood', 'Ġmodule', '.']\n",
      "\n",
      "BPE Token IDs: [35364, 15417, 28403, 318, 753, 756, 305, 1851, 3193, 257, 922, 8265, 13]\n",
      "\n",
      "Decoded Text: Natural Language Processing is incontrovertibly a good module.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Step 1: Load the pre-trained BPE tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nBPE Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nBPE Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ġ Character in Byte-Pair Encoding (BPE)\n",
    "\n",
    "The output above reveals a noticeable difference: the **Ġ** character. In **byte-level BPE**, this character is used to indicate that a word token is preceded by a **space**. This is a crucial part of the tokenization strategy, as it helps BPE models distinguish between different words and their **contexts**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a SentencePiece Model\n",
    "\n",
    "In this section, we'll walk through how to **train a SentencePiece model** from a text corpus using **Byte-Pair Encoding (BPE)**. As seen from above, SentencePiece is a subword tokenization technique that efficiently handles rare or out-of-vocabulary (OOV) words by splitting them into smaller, manageable units.\n",
    "\n",
    "### Training Process Overview:\n",
    "1. **Input Corpus**: We use a text file (e.g., **Shakespeare_1_10.txt**) as input.\n",
    "2. **Model Parameters**:\n",
    "   - **Vocabulary size**: Set to **2000**.\n",
    "   - **Model type**: We use **BPE**.\n",
    "3. **Training**: The model is trained using `SentencePieceTrainer.train()` to learn subword units.\n",
    "4. **Output**: The model and vocabulary files are saved with the specified prefix (e.g., `mymodel.model`, `mymodel.vocab`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in C:\\Users\\Jorda\\Workspace\\nlp_lab_env\\Lib\\site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved with prefix: mymodel\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Step 1: Define the input corpus file (a large text file)\n",
    "corpus_file = 'Shakespear_1_10.txt'  \n",
    "\n",
    "# Step 2: Define the model output directory and parameters\n",
    "model_prefix = 'mymodel' \n",
    "vocab_size = 2000\n",
    "model_type = 'bpe'  # BPE model (could also be 'unigram', 'char', etc.)\n",
    "\n",
    "# Step 3: Train the SentencePiece model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=corpus_file,  \n",
    "    model_prefix=model_prefix,  \n",
    "    vocab_size=vocab_size,  \n",
    "    model_type=model_type, \n",
    "    character_coverage=0.9995,  # Coverage for character set (default is 0.9995)\n",
    "    input_format='text'  # Format of input (usually plain text)\n",
    ")\n",
    "\n",
    "print(f\"Model trained and saved with prefix: {model_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenization and Detokenization\n",
    "\n",
    "Once you’ve trained your **SentencePiece** model, you can use it to tokenize and detokenize sentences. The process involves converting a sentence into subword units (tokens) and then reconstructing the sentence from those tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized sentence: ['▁I', '▁ha', 've', '▁su', 'ccess', 'fu', 'll', 'y', '▁tra', 'in', 'ed', '▁a', '▁S', 'ent', 'en', 'ce', 'P', 'ie', 'ce', '▁mo', 'de', 'l', '.']\n",
      "\n",
      "Detokenized sentence: I have successfully trained a SentencePiece model.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mymodel.model')\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence = \"I have successfully trained a SentencePiece model.\"\n",
    "tokens = sp.encode(sentence, out_type=str)  # or out_type=int for token IDs\n",
    "print(f\"\\nTokenized sentence: {tokens}\")\n",
    "\n",
    "# Detokenize the sentence\n",
    "detokenized = sp.decode(tokens)\n",
    "print(f\"\\nDetokenized sentence: {detokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
