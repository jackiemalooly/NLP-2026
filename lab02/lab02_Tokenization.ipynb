{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: POS Tagging, Morphology, Lemmatization, Dependency Parsing, and Tokenization\n",
    "\n",
    "## Introduction  \n",
    "In this tutorial, we will explore some core NLP tasks using **spaCy**, a powerful and efficient Python library for NLP. Additionally, we will examine tokenization techniques used in modern language models.  \n",
    "\n",
    "### Topics Covered:  \n",
    "\n",
    "1. **Tokenization**  \n",
    "2. **Part-of-Speech (POS) Tagging**  \n",
    "3. **Lemmatization**  \n",
    "4. **Morphology**  \n",
    "5. **Dependency Parsing**  \n",
    "6. **Subword Tokenization**  \n",
    "\n",
    "## Prerequisites  \n",
    "\n",
    "Before we begin, ensure that you have **spaCy** installed in your environment. If you are using the `NLP2026` environment, make sure it is activated. You can install **spaCy** using the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2026/blob/main/lab02/lab02_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "colab_button = HTML(\n",
    "    '<a href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2026/blob/main/lab02/lab02_Tokenization.ipynb\" target=\"_parent\">'\n",
    "    '<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>'\n",
    ")\n",
    "display(colab_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the spaCy model for the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing spaCy\n",
    "Let's start by importing the spaCy library and loading the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning, let's define a couple example strings that we can look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = \"Cats like to chase mice.\"\n",
    "long_text = \"The University of Surrey is a U.K. university founded in 1966, with a budget of Â£314.0 million.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization  \n",
    "\n",
    "**Tokenization** is the process of breaking down text input into smaller units called **tokens**, which can be **words, punctuation marks, or other meaningful elements**. This is a fundamental step in NLP as it enables structured text analysis.  \n",
    "\n",
    "### Tokenization in spaCy  \n",
    "\n",
    "In **spaCy**, tokenization is performed using language-specific grammatical rules. For example:  \n",
    "- Punctuation at the end of a sentence is **split off** as a separate token.  \n",
    "- Abbreviations like **\"U.K.\"** retain their periods within a single token.  \n",
    "\n",
    "### How spaCy Handles Tokenization  \n",
    "\n",
    "- The **input** to the tokenizer is a **Unicode text**.  \n",
    "- The **output** is a **Doc object**, which consists of individual tokens.  \n",
    "- We can **iterate** over tokens and access attributes such as `token.text`.  \n",
    "- spaCy's tokenizer is **non-destructive**, meaning it preserves the original text while providing structured access to tokens.  \n",
    "\n",
    "This efficient tokenization process enables deeper linguistic analysis while maintaining the integrity of the original text.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Library for formatting the table\n",
    "import tabulate\n",
    "\n",
    "doc = nlp(short_text)\n",
    "\n",
    "# Plot table \n",
    "table = []\n",
    "for count, token in enumerate(doc):\n",
    "    table.append([count + 1, token.text])\n",
    "\n",
    "print(tabulate.tabulate(table, headers=['Position','Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again for our longer text\n",
    "doc2 = nlp(long_text)\n",
    "\n",
    "table = []\n",
    "for count, token in enumerate(doc2):\n",
    "    table.append([count + 1, token.text])\n",
    "\n",
    "print(tabulate.tabulate(table, headers=['Position','Text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-Speech (POS) Tagging  \n",
    "\n",
    "**Part-of-Speech (POS) tagging** is the process of assigning grammatical tags to individual words in a sentence, indicating their role, such as **noun, verb, adjective,** etc. This helps in understanding the **syntactic structure** of a sentence and is fundamental in many NLP tasks.  \n",
    "\n",
    "### Using spaCy for POS Tagging  \n",
    "\n",
    "Since we have previously processed the text input using **spaCy**, we can easily retrieve the POS tag for each token with a simple attribute call `token.pos_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our short sentence\n",
    "POS_Tags = []\n",
    "for count, token in enumerate(doc):\n",
    "    POS_Tags.append([count + 1, token.text, token.pos_])\n",
    "\n",
    "print(tabulate.tabulate(POS_Tags, headers=['Position','Text', 'POS Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And our longer sentence\n",
    "POS_Tags = []\n",
    "for count, token in enumerate(doc2):\n",
    "    POS_Tags.append([count + 1, token.text, token.pos_])\n",
    "\n",
    "print(tabulate.tabulate(POS_Tags, headers=['Position','Text', 'POS Tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the example above, we can see several **POS Tags**. Some common examples include:\n",
    "\n",
    "- **DET**: Determiner  \n",
    "- **PROPN**: Proper Noun  \n",
    "- **ADP**: Adposition  \n",
    "\n",
    "These tags represent different parts of speech in a sentence and are crucial for understanding the syntactic structure of the language.\n",
    "\n",
    "### Why Use POS Tagging?\n",
    "\n",
    "In NLP, understanding the grammatical structure of sentences can be extremely valuable for many tasks. POS tagging helps computers to identify the roles that different words play within a sentence, such as subjects, objects, or actions.\n",
    "\n",
    "However, in some tasks, it may also be useful to **discard certain words** based on their POS tags. For example:\n",
    "\n",
    "- **Sentiment Analysis**:  \n",
    "  In sentiment analysis, words like **articles** (e.g., *\"the\"*, *\"a\"*) and **pronouns** (e.g., *\"he\"*, *\"she\"*) might be discarded because they contribute little to the overall sentiment of the text.\n",
    "\n",
    "By filtering out less relevant POS tags, the model can focus on words that carry more meaning and help improve task performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization  \n",
    "\n",
    "**Lemmatization** is the process of reducing words to their **base** or **root** form, known as a **lemma**. This helps in **text normalization** by converting different inflectional forms of a word into a single standardized form.  \n",
    "\n",
    "Lemmatization is particularly useful in NLP tasks such as:  \n",
    "- Improving text **search and retrieval**  \n",
    "- Enhancing **sentiment analysis**  \n",
    "- Reducing **dimensionality** in text-based models  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our short sentence\n",
    "Lemmas = []\n",
    "for count, token in enumerate(doc):\n",
    "    Lemmas.append([count + 1, token.text, token.lemma_])\n",
    "\n",
    "print(tabulate.tabulate(Lemmas, headers=['Position','Text','Lemma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And our longer sentence\n",
    "Lemmas = []\n",
    "for count, token in enumerate(doc2):\n",
    "    Lemmas.append([count + 1, token.text, token.lemma_])\n",
    "\n",
    "print(tabulate.tabulate(Lemmas, headers=['Position','Text','Lemma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Morphology  \n",
    "\n",
    "**Morphology** is the study of the structure of words and their components, such as **prefixes, suffixes,** and **roots**. In essence, it is the process through which the root form (lemma) of a word is modified by the addition of prefixes or suffixes, altering its meaning or grammatical function.  \n",
    "\n",
    "In **spaCy**, we can access detailed morphological information for each token, which includes features such as:  \n",
    "- **Number** (singular or plural)  \n",
    "- **Tense** (present, past, etc.)  \n",
    "- **Mood**: Indicates the mode or manner in which the action is expressed (e.g., **indicative**, **imperative**, or **subjunctive**).  \n",
    "  - Example: *\"She eats\"* (indicative) vs. *\"Eat!\"* (imperative)\n",
    "- **Aspect**: Describes the temporal flow or completion of an action (e.g., **perfective**, **progressive**, or **habitual**).  \n",
    "  - Example: *\"I am eating\"* (progressive) vs. *\"I have eaten\"* (perfective)  \n",
    "\n",
    "This morphological analysis is essential for understanding how words relate to one another in context and is crucial for tasks such as syntactic parsing and word generation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our short sentence\n",
    "Morphs = []\n",
    "for count, token in enumerate(doc):\n",
    "    Morphs.append([count + 1, token.text, token.morph])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text', 'Morphology']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And our longer sentence\n",
    "Morphs = []\n",
    "for count, token in enumerate(doc2):\n",
    "    Morphs.append([count + 1, token.text, token.morph])\n",
    "\n",
    "print(tabulate.tabulate(Morphs, headers=['Position','Text', 'Morphology']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dependency Parsing  \n",
    "\n",
    "Dependency parsing involves analyzing the grammatical structure of a sentence and establishing relationships between **head** words and their **modifiers**. This technique allows us to decompose a sentence into multiple sections, assuming a direct connection between each linguistic unit. These relationships are typically represented as a **tree structure**, illustrating how words depend on one another.  \n",
    "\n",
    "### Example  \n",
    "\n",
    "**Sentence:**  \n",
    "*\"I prefer the morning flight through Denver.\"*  \n",
    "\n",
    "The diagram below visualizes the sentence's dependency structure:  \n",
    "\n",
    "![Dependency Parsing](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/29920Screenshot-127.webp)  \n",
    "*[Source](https://www.analyticsvidhya.com/blog/2021/12/dependency-parsing-in-natural-language-processing-with-examples/)*  \n",
    "\n",
    "### Understanding the Dependency Structure  \n",
    "\n",
    "In the diagram:  \n",
    "\n",
    "- **Directed arcs** illustrate grammatical relationships between words in the sentence.  \n",
    "- The **root** of the tree, *prefer*, serves as the central unit of the sentence.  \n",
    "- Each dependency is labeled with a **dependency tag**, which specifies the relationship between two words.  \n",
    "\n",
    "For instance, in the phrase **\"flight to Denver\"**, the noun *Denver* modifies the meaning of *flight*. This creates a **dependency** where:  \n",
    "\n",
    "- *Flight* is the **head** (governing word).  \n",
    "- *Denver* is the **dependent** (child node).  \n",
    "- This relationship is marked by the **nmod** (nominal modifier) tag, indicating that *Denver* provides additional information about *flight*.  \n",
    "\n",
    "Dependency parsing plays a crucial role in natural language processing (NLP), helping models understand syntactic structures and improving tasks such as named entity recognition, question answering, and machine translation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this can be done easily with spaCy through the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dependenct_Parsing = []\n",
    "for count, token in enumerate(doc):\n",
    "    Dependenct_Parsing.append([count + 1, token.text, token.dep_, [child.text for child in token.children]])\n",
    "\n",
    "print(tabulate.tabulate(Dependenct_Parsing, headers=['Position','Text', 'Dependency', 'Children']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table might look very confusing, which is why spaCy offers a quick way to easily view the tree structure with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dependenct_Parsing = []\n",
    "for count, token in enumerate(doc2):\n",
    "    Dependenct_Parsing.append([count + 1, token.text, token.dep_, [child.text for child in token.children]])\n",
    "\n",
    "print(tabulate.tabulate(Dependenct_Parsing, headers=['Position','Text', 'Dependency', 'Children']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table even more so, but luckily we can visualise it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a more complicated one\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc2, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subword Tokenization  \n",
    "\n",
    "Tokenization is the process of breaking down a sentence into smaller units, enabling AI models to process text as discrete tokens rather than as a continuous block of text. In previous sections, you have used spaCy for tokenization, which primarily segments text into individual words. While this approach is efficient, it struggles with handling uncommon or out-of-vocabulary (OOV) words.  \n",
    "\n",
    "To address this limitation, modern tokenization techniques predominantly use **subword-based methods**. Instead of strictly segmenting text into words, these approaches break words into smaller subword units when necessary. For example, the word *unhappiness* might be tokenized into *un* and *happiness*. This strategy offers several advantages:  \n",
    "\n",
    "- **Improved Handling of Rare Words** â€“ By decomposing words into meaningful subunits, the model can recognize and generate words that were not explicitly seen during training.  \n",
    "- **Compact Vocabulary** â€“ Instead of storing an extensive vocabulary of all possible words, subword tokenization relies on a smaller set of subunits, which can be combined to form complex words.  \n",
    "- **Efficient Representation** â€“ By balancing whole-word tokens with subword segments, this method optimizes both memory usage and model performance.  \n",
    "\n",
    "(**Note**: Often tokenizers try to maintain words that are frequently used, and split rare words into smaller subwords)\n",
    "\n",
    "We will therefore explore three subword tokenization techniques:  \n",
    "\n",
    "1. **WordPiece**  \n",
    "2. **SentencePiece** \n",
    "3. **Byte-Pair Encoding (BPE)**  \n",
    "\n",
    "These tokenization methods have become standard in modern NLP models and are widely used in recent Large Language Models (LLMs).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's define a simple string that we will be tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is incontrovertibly a good module.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. WordPiece Tokenization\n",
    "\n",
    "**WordPiece Tokenization** is a subword tokenization technique used in models like BERT (Bidirectional Encoder Representations from Transformers). It breaks down words into subwords, that can efficiently handle complex words, unknown terms, or out-of-vocabulary (OOV) words.\n",
    "\n",
    "WordPiece works by iteratively merging the most frequent pairs of characters or subword units in a large corpus. The resulting subwords represent the language's most frequent word components, which helps to reduce the size of the vocabulary while maintaining full language coverage.\n",
    "\n",
    "### Example: Tokenizing Text with BERTâ€™s WordPiece Tokenizer\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to showcase how the BERT tokenizer works. Weâ€™ll tokenize a sample sentence, convert the tokens into token IDs, and then decode those IDs back into a human-readable string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install the required library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary module from Hugging Face transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Step 1: Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nBERT Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nBERT Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the word **\"incontrovertibly\"**, which is quite rare, is split into **5 subwords** by the tokenizer. This process of splitting words into smaller subunits is particularly useful for handling rare or out-of-vocabulary (OOV) words.\n",
    "\n",
    "Each subword is represented as a **token**, and you can see that certain tokens are prefixed with `##`. This notation indicates that these subwords are continuations of a previous subword (i.e., they are not starting a new token). The tokenizer has broken down the word into smaller, more frequent subwords that are part of the modelâ€™s vocabulary.\n",
    "\n",
    "### Why Do We Use Token IDs?\n",
    "\n",
    "As shown above, the tokens are also associated with **token IDs**. These token IDs are numerical representations of the words or subwords. In the context of machine learning and NLP models, it's crucial to convert words into numbers because models operate on numerical data.\n",
    "\n",
    "Each token is mapped to a unique ID in the modelâ€™s vocabulary, which allows the model to process text efficiently. This conversion is essential because:\n",
    "\n",
    "- **Models can't understand raw text**: Machine learning models, including NLP models, don't process text directly. Instead, they process **numerical representations** of words.\n",
    "- **Token IDs map to model parameters**: The model's vocabulary is essentially a map of tokens (words or subwords) to unique IDs. These IDs are used by the model to look up the corresponding word embeddings (vector representations) in the modelâ€™s parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. SentencePiece Tokenization\n",
    "\n",
    "**SentencePiece** is another popular subword tokenization technique used in models like T5 (Text-to-Text Transfer Transformer) and other transformer-based architectures.\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to demonstrate how the **SentencePiece tokenizer** works. We will also be training our own SentencePiece tokenizer later on!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Step 1: Load a pre-trained SentencePiece tokenizer (T5 model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens using SentencePiece\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nSentencePiece Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nSentencePiece Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Byte-Pair Encoding (BPE) Tokenization\n",
    "\n",
    "**Byte-Pair Encoding (BPE)** is also another subword tokenization technique used in models like GPT (Generative Pretrained Transformer) and other transformer-based architectures. \n",
    "\n",
    "### Byte-Level BPE Tokenization\n",
    "\n",
    "Instead of treating text as sequences of **Unicode characters** (such as 'a', 'b', 'c', etc.), **byte-level BPE** tokenizes text at the **byte level**. Each character, word, and symbol is first converted into its corresponding **byte representation**.\n",
    "\n",
    "The **base vocabulary** for byte-level BPE is much smaller, consisting of only **256 byte values**, as there are 256 possible byte values. This ensures that any character can be represented without needing to resort to an **unknown token** for out-of-vocabulary (OOV) words.\n",
    "\n",
    "This approach allows models like **GPT-2** and **RoBERTa** to handle any character or symbol, including those from different languages, special symbols, or rare characters, without needing additional vocabularies or dealing with OOV issues.\n",
    "\n",
    "\n",
    "### Example: Tokenizing Text with BPE Tokenizer\n",
    "\n",
    "In this section, we will use the Hugging Face `transformers` library to demonstrate how a BPE tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Step 1: Load the pre-trained BPE tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 2: Tokenize the text into subword tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nBPE Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Convert tokens to their corresponding token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nBPE Token IDs:\", token_ids)\n",
    "\n",
    "# Step 4: Decode the token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ä  Character in Byte-Pair Encoding (BPE)\n",
    "\n",
    "The output above reveals a noticeable difference: the **Ä ** character. In **byte-level BPE**, this character is used to indicate that a word token is preceded by a **space**. This is a crucial part of the tokenization strategy, as it helps BPE models distinguish between different words and their **contexts**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a SentencePiece Model\n",
    "\n",
    "In this section, we'll walk through how to **train a SentencePiece model** from a text corpus using **Byte-Pair Encoding (BPE)**. As seen from above, SentencePiece is a subword tokenization technique that efficiently handles rare or out-of-vocabulary (OOV) words by splitting them into smaller, manageable units.\n",
    "\n",
    "### Training Process Overview:\n",
    "1. **Input Corpus**: We use a text file (e.g., **Shakespeare_1_10.txt**) as input.\n",
    "2. **Model Parameters**:\n",
    "   - **Vocabulary size**: Set to **2000**.\n",
    "   - **Model type**: We use **BPE**.\n",
    "3. **Training**: The model is trained using `SentencePieceTrainer.train()` to learn subword units.\n",
    "4. **Output**: The model and vocabulary files are saved with the specified prefix (e.g., `mymodel.model`, `mymodel.vocab`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Step 1: Define the input corpus file (a large text file)\n",
    "corpus_file = 'Shakespear_1_10.txt'  \n",
    "\n",
    "# Step 2: Define the model output directory and parameters\n",
    "model_prefix = 'mymodel' \n",
    "vocab_size = 2000\n",
    "model_type = 'bpe'  # BPE model (could also be 'unigram', 'char', etc.)\n",
    "\n",
    "# Step 3: Train the SentencePiece model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=corpus_file,  \n",
    "    model_prefix=model_prefix,  \n",
    "    vocab_size=vocab_size,  \n",
    "    model_type=model_type, \n",
    "    character_coverage=0.9995,  # Coverage for character set (default is 0.9995)\n",
    "    input_format='text'  # Format of input (usually plain text)\n",
    ")\n",
    "\n",
    "print(f\"Model trained and saved with prefix: {model_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenization and Detokenization\n",
    "\n",
    "Once youâ€™ve trained your **SentencePiece** model, you can use it to tokenize and detokenize sentences. The process involves converting a sentence into subword units (tokens) and then reconstructing the sentence from those tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mymodel.model')\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence = \"I have successfully trained a SentencePiece model.\"\n",
    "tokens = sp.encode(sentence, out_type=str)  # or out_type=int for token IDs\n",
    "print(f\"\\nTokenized sentence: {tokens}\")\n",
    "\n",
    "# Detokenize the sentence\n",
    "detokenized = sp.decode(tokens)\n",
    "print(f\"\\nDetokenized sentence: {detokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸ”Ž Interactive Exploration\n",
    "\n",
    "You can explore tokenization with various models interactively here:\n",
    "\n",
    "**Tiktoken Visualizer:**  \n",
    "ðŸ‘‰ https://tiktokenizer.vercel.app\n",
    "\n",
    "\n",
    "\n",
    "Try pasting the following examples into the tool and compare the token counts:\n",
    "\n",
    "<ol>\n",
    "  <li>Cats like to chase mice.</li>\n",
    "  <li>The University of Surrey was founded in 1966 with Â£314 million.</li>\n",
    "  <li>incontrovertibly</li>\n",
    "  <li>emojis ðŸ™‚ðŸ™ƒðŸ”¥</li>\n",
    "</ol>\n",
    "\n",
    "See how:\n",
    "\n",
    "<ol>\n",
    "  <li>Whitespace affects tokenization.</li>\n",
    "  <li>Some words are split into subword pieces</li>\n",
    "  <li>Emojis and symbols are handled</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
